{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Raspbernetes","text":"<p>This repo is a declarative implementation of a Kubernetes cluster which follows GitOps princples. It's using the FluxCD.</p>"},{"location":"#mission","title":"Mission","text":"<p>The goal is to demonstrates how to implement enterprise-grade security, observability, and overall cluster config management using GitOps in a Kubernetes cluster.</p>"},{"location":"#story","title":"Story","text":"<p>This project ...</p>"},{"location":"configuration/api-access/","title":"Access Kubernetes API through Cloudflare","text":""},{"location":"configuration/api-access/#prerequisites","title":"Prerequisites","text":"<p>Before proceeding, make sure you have the following prerequisites in place to set up your kubeconfig locally.</p>"},{"location":"configuration/api-access/#setup-kubeconfig-locally","title":"Setup Kubeconfig Locally","text":"<p>To configure the kubeconfig correctly, follow the steps below:</p> <ol> <li> <p>Go to https://login.raspbernetes.com in your web browser.</p> </li> <li> <p>Once authenticated, the webpage will provide command prompts specific to your account. These commands need to be set up on the client machine that you will be using with kubectl to run commands.</p> </li> </ol> <p>Note: The commands provided will configure the necessary authentication and context for your kubeconfig, enabling seamless interaction with the Kubernetes cluster.</p> <p>By following these steps, you will set up the kubeconfig correctly on your local machine, allowing you to access and manage the Kubernetes cluster using kubectl commands.</p>"},{"location":"configuration/api-access/#connect-from-a-client-machine","title":"Connect From a Client Machine","text":"<p>To enable remote kubectl access to the Kubernetes cluster, follow the instructions below.</p> <p>Note: These instructions are applicable if you are part of the Raspbernetes project and have been granted access by an admin.</p>"},{"location":"configuration/api-access/#1-install-cloudflared-on-the-client-machine","title":"1. Install Cloudflared on the Client Machine","text":"<p>Download and install cloudflared on the client machine that will connect to the Kubernetes cluster. You can find the installation instructions HERE.</p> <p><code>Cloudflared</code> will need to be installed on each user device that will connect to the kube-apiserver.</p>"},{"location":"configuration/api-access/#2-establish-connection","title":"2. Establish Connection","text":"<p>Run the following command to create a connection from the device to Cloudflare. Any available port can be specified.</p> <pre><code>$ cloudflared access tcp --hostname api.raspbernetes.com --url 127.0.0.1:1234\n</code></pre> <p>With this service running, you can run a <code>kubectl</code> command and <code>cloudflared</code> will launch a browser window and prompt the user to authenticate with the Github SSO provider. Once authenticated, <code>cloudflared</code> will expose the connection to the client machine at the local URL specified in the command.</p> <p><code>kubeconfig</code> does not support proxy command configurations at this time, though the community has submitted plans to do so. In the interim, users can alias the cluster's API server to save time.</p> <pre><code>$ alias kubeone=\"env HTTPS_PROXY=socks5://127.0.0.1:1234 kubectl\"\n</code></pre>"},{"location":"configuration/api-access/#3-test-connection","title":"3. Test Connection","text":"<p>To test the connection, use the alias and run a kubectl command. For example:</p> <pre><code>kubeone get nodes\n</code></pre> <p>If the connection is successful, you should see the appropriate information about the cluster's nodes.</p> <p>Example result:</p> <pre><code>NAME            STATUS   ROLES            AGE   VERSION\nk8s-master-01   Ready    control-plane    8h    v1.26.1\nk8s-master-02   Ready    control-plane    8h    v1.26.1\nk8s-master-03   Ready    control-plane    8h    v1.26.1\nk8s-worker-01   Ready    &lt;none&gt;           8h    v1.26.1\n</code></pre> <p>You now have complete access to the cluster using the set alias. Please ensure that the cluster has RBAC enabled and that you have been granted the necessary user permissions by an admin.</p> <p>Note: Official documentation can also be referenced HERE</p>"},{"location":"configuration/ip-allocation/","title":"IP Allocation","text":"<p>The IP Allocation section provides an overview of the allocated IP addresses for different applications, nodes, and external devices within my environment.</p>"},{"location":"configuration/ip-allocation/#kubernetes-nodes","title":"Kubernetes Node(s)","text":"<p>The Kubernetes Node(s) table displays the IP addresses assigned to each node in the cluster:</p> Node/Instance Type IP/CIDR Control Plane VIP 192.168.50.200/32 Protectli FW2B 01 Control Plane 192.168.50.111/32 Protectli FW2B 02 Control Plane 192.168.50.112/32 Protectli FW2B 03 Control Plane 192.168.50.113/32 Protectli VP2410 01 Node 192.168.50.114/32 Protectli VP2410 02 Node 192.168.50.115/32 Protectli VP2410 03 Node 192.168.50.116/32 Raspberry Pi 01 Node 192.168.50.121/32 Raspberry Pi 02 Node 192.168.50.122/32 Raspberry Pi 03 Node 192.168.50.123/32 Raspberry Pi 04 Node 192.168.50.124/32 Rock Pi 01 Node 192.168.50.131/32 Rock Pi 02 Node 192.168.50.132/32 Rock Pi 03 Node 192.168.50.133/32"},{"location":"configuration/ip-allocation/#kubernetes-applications","title":"Kubernetes Application(s)","text":"<p>The Kubernetes Application(s) table presents the IP addresses allocated to different applications in the Kubernetes cluster:</p> Application/Node Type IP/CIDR Metallb Daemonset 192.168.50.150 &lt;-&gt; 192.168.50.155 Istio Ingress LoadBalancer 192.168.50.180/32 Coredns LoadBalancer 192.168.50.181/32 Mosquitto LoadBalancer 192.168.50.182/32 Zigbee2mqtt LoadBalancer 192.168.50.183/32 Nginx Ingress LoadBalancer 192.168.50.180/32 K8s Gateway LoadBalancer 192.168.50.188/32 Blocky LoadBalancer 192.168.50.191/32"},{"location":"configuration/ip-allocation/#external-devices","title":"External Device(s)","text":"<p>The External Device(s) table lists IP addresses assigned to devices outside the Kubernetes cluster:</p> Application Type IP/CIDR Zigbee Controller N/A 192.168.50.165/32 Ender 5 Pro 3D Printer N/A x.x.x.x/32"},{"location":"configuration/repo-structure/","title":"Flux Repository Structure","text":"<p>Work in progress</p> <p>This document is a work in progress.</p>"},{"location":"configuration/repo-structure/#tldr-quick-start","title":"TL;DR Quick Start","text":"<p>If you're familiar with Kustomize and how it operates within the Flux ecosystem this will provide a quick overview:</p> <pre><code>.\n\u2514\u2500\u2500 kubernetes/\n    \u251c\u2500\u2500 clusters/\n    \u2502   \u251c\u2500\u2500 production/                         # One folder per cluster.\n    \u2502   \u2502   \u251c\u2500\u2500 flux-system/                    # Folder containing flux-system manifests.\n    \u2502   \u2502   \u2502   \u251c\u2500\u2500 ...                         # Flux component resource manifests.\n    \u2502   \u2502   \u2502   \u2514\u2500\u2500 kustomization.yaml          # Generated kustomization per cluster bootstrap.\n    \u2502   \u2502   \u2514\u2500\u2500 kustomization.yaml              # Kustomization per cluster referring all manifests in core and namespace directory.\n    \u2502   \u2514\u2500\u2500 staging/\n    \u2502       \u251c\u2500\u2500 flux-system/\n    \u2502       \u2502   \u251c\u2500\u2500 ...\n    \u2502       \u2502   \u2514\u2500\u2500 kustomization.yaml\n    \u2502       \u2514\u2500\u2500 kustomization.yaml\n    \u251c\u2500\u2500 core/\n    \u2502   \u251c\u2500\u2500 base/\n    \u2502   \u2502   \u2514\u2500\u2500 .../                            # One folder per resource type and/or app with its core dependency with prune disabled.\n    \u2502   \u2502       \u2514\u2500\u2500 application/                # One folder per application with core manifests.\n    \u2502   \u2502           \u2514\u2500\u2500 kustomization.yaml      # Kustomization per core application.\n    \u2502   \u2514\u2500\u2500 overlays/\n    \u2502       \u251c\u2500\u2500 production/\n    \u2502       \u2502   \u251c\u2500\u2500 kustomization.yaml          # Kustomization per cluster referencing each core app required.\n    \u2502       \u2502   \u2514\u2500\u2500 patch.yaml                  # Optional patch for each environment.\n    \u2502       \u2514\u2500\u2500 staging/\n    \u2502           \u251c\u2500\u2500 kustomization.yaml\n    \u2502           \u2514\u2500\u2500 patch.yaml\n    \u2514\u2500\u2500 namespaces/\n        \u251c\u2500\u2500 base/\n        \u2502   \u2514\u2500\u2500 namespace/                      # One folder per namespace containing base resources.\n        \u2502       \u251c\u2500\u2500 namespace.yaml              # Namespace manifest.\n        \u2502       \u251c\u2500\u2500 kustomization.yaml          # Kustomization per namespace referring all manifests in this current directory.\n        \u2502       \u2514\u2500\u2500 application/                # Folder per app containing manifests and patches for each application.\n        \u2502           \u2514\u2500\u2500 kustomizaiton.yaml      # Kustomization per app referring all manifests in this directory.\n        \u2514\u2500\u2500 overlays/\n            \u251c\u2500\u2500 production/\n            \u2502   \u251c\u2500\u2500 kustomization.yaml          # Kustomization per cluster referencing each namespace and app required.\n            \u2502   \u2514\u2500\u2500 patch.yaml                  # Optional patch for each environment.\n            \u2514\u2500\u2500 staging/\n                \u251c\u2500\u2500 kustomization.yaml\n                \u2514\u2500\u2500 patch.yaml\n</code></pre>"},{"location":"configuration/repo-structure/#repository-structure-breakdown","title":"Repository Structure Breakdown","text":"<p>This Git repository contains the following directories:</p> <ul> <li>clusters dir contains the Flux configuration per cluster.</li> <li>core dir contains cluster resources that are core prerequisites to the cluster.</li> <li>namespaces dir contains namespaces and application workloads per cluster.</li> </ul> <pre><code>.\n\u251c\u2500\u2500 clusters/\n\u2502   \u251c\u2500\u2500 production\n\u2502   \u2514\u2500\u2500 staging\n\u251c\u2500\u2500 core/\n\u2502   \u251c\u2500\u2500 base\n\u2502   \u2514\u2500\u2500 overlays/\n\u2502       \u251c\u2500\u2500 production\n\u2502       \u2514\u2500\u2500 staging\n\u2514\u2500\u2500 namespaces/\n    \u251c\u2500\u2500 base\n    \u2514\u2500\u2500 overlays/\n        \u251c\u2500\u2500 production\n        \u2514\u2500\u2500 staging\n</code></pre> <p>The clusters/ dir contains configuration for each cluster definition and the infrastructure as code for each relevant cluster where applicable.</p> <p>The core/ dir contains all resources that are prerequisites to namespaces and workloads, this includes resources: CRDs, certain applications like Istio and Gatekeeper that must exist prior to other workloads, and crossplane resources that provisions infrastructure.</p> <p>The namespaces/ configuration is structured into:</p> <ul> <li>namespaces/base/ dir contains namespaces and application workload resources.</li> <li>namespaces/overlays/production/ dir contains the production cluster values and references what base components to deploy.</li> <li>namespaces/overlays/staging/ dir contains the stating cluster values and references what base components to deploy.</li> </ul> <pre><code>.\n\u2514\u2500\u2500 namespaces/\n    \u251c\u2500\u2500 base/\n    \u2502   \u2514\u2500\u2500 namespace/\n    \u2502       \u251c\u2500\u2500 namespace.yaml\n    \u2502       \u251c\u2500\u2500 kustomization.yaml\n    \u2502       \u2514\u2500\u2500 application/\n    \u2502           \u251c\u2500\u2500 helmrelease.yaml\n    \u2502           \u2514\u2500\u2500 kustomizaiton.yaml\n    \u2514\u2500\u2500 overlays/\n        \u251c\u2500\u2500 production/\n        \u2502   \u251c\u2500\u2500 kustomization.yaml\n        \u2502   \u2514\u2500\u2500 patch.yaml\n        \u2514\u2500\u2500 staging/\n            \u2514\u2500\u2500 ...\n</code></pre> <p>In namespaces/base/ dir will be a hierarchy of all namespace/ dirs which will contain application resources. Each cluster overlay includes each namespace and/or application which is explicitly referenced; The base application configuration is defined with the following values:</p> <pre><code>apiVersion: helm.toolkit.fluxcd.io/v2beta2\nkind: HelmRelease\nmetadata:\n  name: metallb\n  namespace: network-system\nspec:\n  interval: 5m\n  chart:\n    spec:\n      chart: metallb\n      version: 2.0.4\n      sourceRef:\n        kind: HelmRepository\n        name: bitnami-charts\n        namespace: flux-system\n      interval: 10m\n  values:\n    configInline:\n      address-pools:\n        - name: default\n          protocol: layer2\n          addresses:\n            - 192.168.1.150-192.168.1.155\n</code></pre> <p>In namespaces/overlays/production/ dir we have a Kustomize patch file(s) with the production cluster specific values:</p> <pre><code>apiVersion: helm.toolkit.fluxcd.io/v2beta2\nkind: HelmRelease\nmetadata:\n  name: metallb\n  namespace: network-system\nspec:\n  values:\n    configInline:\n      address-pools:\n        - name: default\n          protocol: layer2\n          addresses:\n            - 192.168.1.150-192.168.1.155\n</code></pre> <p>Note that whilst using Kustomize we can overwrite default values; in this example the default MetalLB address pool will be patched in the production cluster to a unique pool.</p>"},{"location":"configuration/sealed-secrets/","title":"Sealed Secrets","text":"<p>Work in progress</p> <p>This document is a work in progress.</p> <p>When bootstrapping your cluster for the first time you must store a unique public &amp; private key for your sealed-secrets controller to use for managing your sensitive keys and passwords in your Kubernetes cluster.</p>"},{"location":"configuration/sealed-secrets/#install-the-cli-tool","title":"Install the CLI tool","text":"<p>For all installation methods visit the Sealed Secrets install guide</p> <pre><code>brew install kubeseal\n</code></pre>"},{"location":"configuration/sealed-secrets/#remove-existing-key-cert","title":"Remove Existing Key &amp; Cert","text":"<p>Remove the sealed-secrets master key currently present in this repository</p> <pre><code>rm -f kubernetes/clusters/&lt;cluster&gt;/secrets/sealed-secret-private-key.enc.yaml\n</code></pre> <p>Remove the sealed-secrets public cert currently present in this repository</p> <pre><code>rm -f kubernetes/clusters/&lt;cluster&gt;/secrets/sealed-secret-public-cert.yaml\n</code></pre>"},{"location":"configuration/sealed-secrets/#store-new-key-cert","title":"Store New Key &amp; Cert","text":""},{"location":"configuration/sealed-secrets/#private-key","title":"Private Key","text":"<p>Once sealed-secrets has been re-deployed to a running cluster you must store the private key and public cert in this repository.</p> <p>Get the generated sealed-secret private key</p> <pre><code>kubectl get secret -n kube-system -l sealedsecrets.bitnami.com/sealed-secrets-key -o yaml &gt; kubernetes/clusters/&lt;cluster&gt;/secrets/sealed-secret-private-key.yaml\n</code></pre> <p>Encrypt the sealed-secret private key using SOPs</p> <pre><code>sops --encrypt kubernetes/clusters/&lt;cluster&gt;/secrets/sealed-secret-private-key.yaml &gt; kubernetes/clusters/&lt;cluster&gt;/secrets/sealed-secret-private-key.enc.yaml\n</code></pre> <p>Remove unencrypted private key</p> <pre><code>rm -f kubernetes/clusters/&lt;cluster&gt;/secrets/sealed-secret-private-key.yaml\n</code></pre>"},{"location":"configuration/sealed-secrets/#public-cert","title":"Public Cert","text":"<p>Fetch the generated sealed-secret public cert and store it</p> <pre><code>kubeseal \\\n    --controller-name sealed-secrets \\\n    --fetch-cert &gt; kubernetes/clusters/&lt;cluster&gt;/secrets/sealed-secret-public-cert.yaml\n</code></pre>"},{"location":"configuration/sealed-secrets/#encrypt-secrets","title":"Encrypt Secrets","text":"<p>With a newly generated private key from sealed-secrets you will need to re-encrypt all of the existing required secrets.</p> <p>Create an alias for the CLI tool (recommended)</p> <pre><code>alias kubeseal='kubeseal --cert kubernetes/clusters/&lt;cluster&gt;/secrets/sealed-secret-public-cert.pem --controller-name sealed-secrets --format yaml'\n</code></pre> <p>Encrypt new Kubernetes secret</p> <pre><code>kubeseal &lt; secret.yaml &gt; secret.encrypted.yaml\n</code></pre> <p>Remove the unencrypted secret</p> <p>You must encrypt your secrets with the correct cluster public certificate. For more in-depth instructions the official docs can be found here</p>"},{"location":"configuration/sealed-secrets/#offline-decryption","title":"Offline Decryption","text":"<p>Storing the private key allows an offline decryption, this is not recommended and should only be used in a break-glass scenario when the cluster is down and secrets must be accessed.</p> <p>Unencrypt the sealed-secret private key</p> <pre><code>sops --decrypt kubernetes/clusters/&lt;cluster&gt;/secrets/sealed-secret-private-key.enc.yaml -oyaml &gt; kubernetes/clusters/&lt;cluster&gt;/secrets/sealed-secret-private-key.yaml\n</code></pre> <p>Unseal the encrypted secret(s)</p> <pre><code>kubeseal --recovery-unseal --recovery-private-key ./kubernetes/clusters/&lt;cluster&gt;/secrets/sealed-secret-private-key.yaml &lt; &lt;path-to-file&gt;/secret.encrypted.yaml\n</code></pre> <p>Re-Encrypt the sealed-secret private key</p> <pre><code>sops --encrypt kubernetes/clusters/&lt;cluster&gt;/secrets/sealed-secret-private-key.yaml &gt; kubernetes/clusters/&lt;cluster&gt;/secrets/sealed-secret-private-key.enc.yaml\n</code></pre> <p>Remove the unencrypted private key</p> <pre><code>rm -f kubernetes/clusters/&lt;cluster&gt;/secrets/sealed-secret-private-key.yaml\n</code></pre>"},{"location":"contributing/","title":"Contributing","text":"<p>We warmly welcome and appreciate all contributions to this project! Whether you're a developer, designer, tester, or documentation enthusiast, your skills and ideas are valuable to us. There is the official contributing guide which can be found HERE.</p>"},{"location":"contributing/#contributors","title":"Contributors","text":"<p>This project owes its success to the incredible individuals who contribute their time, expertise, and passion.</p> <p> </p> <p>Every line of code, bug report, documentation improvement, or idea shared has played a part in shaping this project. We deeply appreciate the dedication and effort of our contributors.</p> <p>Join us on this exciting journey, and let's create something remarkable together! Don't hesitate to get involved and make your mark.</p> <p>Watch this space for the upcoming official contribution guide, which will provide more details on how you can contribute to this project.</p> <p>Thank you for being a part of our vibrant community!</p>"},{"location":"faq/","title":"Frequently Asked Questions","text":""},{"location":"faq/#fluxcd-vs-argocd","title":"FluxCD vs. ArgoCD","text":"<p>FluxCD and ArgoCD are both popular GitOps tools used for managing and deploying applications in Kubernetes clusters. However, there are some key differences between the two.</p> <p>FluxCD, as a GitOps tool, is built with multi-tenancy, multi-cluster, and multi-cloud environments in mind from the ground up. It provides robust support for managing applications across different tenants, clusters, and cloud platforms, making it highly flexible and scalable. FluxCD offers a rich set of features and capabilities that cater to complex enterprise scenarios.</p> <p>It's important to note that both FluxCD and ArgoCD have their strengths and suitability for different use cases. The choice between the two ultimately depends on the specific requirements and preferences of the organization or project at hand.</p>"},{"location":"installation/","title":"Getting Started","text":"<p>These instructions will assume you have an Kubernetes cluster and want to bootstrap this GitOps repository to it.</p>"},{"location":"installation/#install-the-cli-tool","title":"Install the CLI tool","text":"<p>For all installation methods visit the Flux install guide</p> <pre><code>brew install fluxcd/tap/flux\n</code></pre>"},{"location":"installation/#install-flux","title":"Install Flux","text":"<p>For the full installation guide visit the Flux bootstrap guide</p> <p>Validate the cluster and its connectivity</p> <pre><code>kubectl cluster-info\n</code></pre> <p>Export your GitHub personal access token, username, repository and cluster</p> <pre><code>export GITHUB_TOKEN=&lt;your-token&gt;\nexport GITHUB_USER=&lt;your-username&gt;\nexport GITHUB_REPO=&lt;your-repo&gt;\nexport CLUSTER=&lt;target-cluster&gt;\n</code></pre> <p>Verify that your cluster satisfies the prerequisites</p> <pre><code>flux check --pre\n</code></pre> <p>Run the bootstrap command to install Flux</p> <pre><code>flux bootstrap github \\\n  --owner=\"${GITHUB_USER}\" \\\n  --repository=\"${GITHUB_REPO}\" \\\n  --path=kubernetes/clusters/\"${CLUSTER}\" \\\n  --branch=main \\\n  --personal\n</code></pre> <p>Note: If you have network issues with Flux starting you may need to set <code>--network-policies=false</code> in the bootstrap command.</p> <p>You may also use the automated installation script - Either override the defaults in the install script or as environment variables.</p>"},{"location":"sponsor/","title":"Sponsors","text":"<p>If you have found value in any of my open-source projects, I would greatly appreciate your support as a sponsor. Don't forget to star the repository as well!</p> <p>To become a sponsor, please visit my official GitHub sponsor page HERE.</p>"},{"location":"sponsor/#supporters","title":"Supporters","text":"<p>Thank you to all our backers and sponsors! Your contributions are immensely valuable and greatly appreciated.</p> <p></p> <p> </p>"},{"location":"sponsor/#premium-sponsors","title":"Premium Sponsors","text":"<p>I would like to extend a special thank you to our premium sponsors, whose generous support makes a significant impact on the continued development and success of this project.</p> <ul> <li>Bitscope</li> <li>Rock Pi</li> <li>Protectli</li> <li>Noctua</li> </ul> <p>Your sponsorship plays a crucial role in supporting ongoing enhancements and the sustainability of this project. We are incredibly grateful for your generous contributions!</p> <p>To all our backers and sponsors, your support enables us to maintain and improve this project, benefiting the entire community. Thank you for being a part of our journey!</p>"}]}